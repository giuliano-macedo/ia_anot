{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Anotações da disciplina de Inteligência Artificial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalização**\n",
    "* *Note que, essas operações devem ser resolvidas obtendo os valores linha a linha*;\n",
    "* Obter a média dos termos(**np.mean()**):\n",
    "    * $\\text{Média: }\\mu_k=\\frac{1}{m}\\sum\\limits_{i=1}^{m}x_k^{(i)}$ *(Onde 'm' é a quantidade das linhas)*\n",
    "\n",
    "* Calcular o desvio padrão dos termos(**np.std()**):\n",
    "    * $\\text{Desvio Padrão: }\\sigma_k=\\sqrt{\\frac{1}{m}\\sum\\limits_{i=1}^{m}(x_k^{(i)}-\\mu_k)^2}$\n",
    "\n",
    "* Atribuir o \"novo\" valor de X para cada X calculado:\n",
    "    * $X_k=\\frac{x_k-\\mu_k}{\\sigma_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regressão Linear**\n",
    "\n",
    "### Observações\n",
    "* $J(h)$ = *Função de Perda*\n",
    "\n",
    "* *Custo* seria essa mesma função, porém, somente para um valor;\n",
    "\n",
    "* **Logo, a função de perda é a soma dos custos!**\n",
    "\n",
    "### Notação Algébrica\n",
    "\n",
    "Definições:\n",
    "* $h(x)=\\theta_1x+\\theta_0$\n",
    "    \n",
    "* $J(h)=\\frac{1}{2m}\\sum\\limits_{i=0}^{m}((h(x_i)-y_i)^2)$    \n",
    "\n",
    "    Algoritmo; sendo $i$ a iteração:\n",
    "\n",
    "    * $\\theta^i_1= \\theta_1^{i-1} - \\alpha \\frac{\\partial}{\\partial \\theta_1^{i-1}}J \\to ~\n",
    "     \\theta_1^{i-1} - \\alpha \\frac{1}{m}\\sum\\limits_{i=0}^{m}((h(x_i)-y_i)x)$\n",
    "    * $\\theta^i_0= \\theta_0^{i-1} - \\alpha \\frac{\\partial}{\\partial \\theta_0^{i-1}}J \\to ~\n",
    "    \\ \\theta_0^{i-1} - \\alpha \\frac{1}{m}\\sum\\limits_{i=0}^{m}((h(x_i)-y_i)1)$\n",
    "    \n",
    "\n",
    "### Notação Matricial\n",
    "\n",
    "Definições:\n",
    "\n",
    "Considere $x'$ a entrada, então:\n",
    "\n",
    "* $X=\\begin{bmatrix}\n",
    "        1 & x'_1 \\\\\n",
    "        1 & x'_2 \\\\\n",
    "        1 & .. \\\\\n",
    "        1 & x'_m \\\\\n",
    "    \\end{bmatrix}$ ,\n",
    "* $\\Theta=\\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\n",
    "    \\end{bmatrix}$\n",
    "    \n",
    "    O algoritmo é representado por:\n",
    "    \n",
    "    * $H=X \\cdot \\Theta$\n",
    "    * $E=H - Y$\n",
    "    * $J_i=\\frac{1}{2m}(E^T \\cdot E)$\n",
    "    * $\\Theta=\\Theta - \\frac{\\alpha}{m}(X^T \\cdot E)$    \n",
    "    \n",
    "    Sendo $i$ a iteração;\n",
    "\n",
    "### Múltiplas Variáveis\n",
    "$X=\\begin{bmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & x^{(1)}_3 \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & x^{(2)}_3 \\\\\n",
    "1 & x^{(3)}_1 & x^{(3)}_2 & x^{(3)}_3 \\\\\n",
    "\\end{bmatrix},\n",
    "\\Theta=\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\theta_3\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de avaliação de regressão\n",
    "\n",
    "Aqui estão três métricas de avaliação comuns para problemas de regressão:\n",
    "\n",
    "Onde $n$ é quantidade de *linhas* da matriz.\n",
    "\n",
    "**Mean absolute error**(erro absoluto médio) (MAE) é a média do valor absoluto dos erros:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error**(erro médio quadrático) (MSE) é a média dos erros quadrados:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Square Error**(raiz do erro quadrático médio) (RMSE) é a raiz quadrada da média dos erros quadrados:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "Comparando estas métricas:\n",
    "\n",
    "- **MAE** é o mais fácil de entender, porque é o erro médio.\n",
    "- **MSE** é mais popular que o MAE, porque a MSE \"puniria\" erros maiores, o que tende a ser útil no mundo real.\n",
    "- **RMSE** é ainda mais popular do que MSE, porque o RMSE é interpretável nas unidades \"y\".\n",
    "\n",
    "Todas estas são **funções de perda**, porque queremos minimizá-las."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regressão Logística**\n",
    "\n",
    "### Observações\n",
    "* Na Regressão Logística, temos a **Função sigmóide** sendo um componente de $H_{\\theta}$.\n",
    "\n",
    "* $\\hat y$ = $\\textit{y predito}$ = $h(x)$ = $H_{\\theta}$.\n",
    "\n",
    "$X,Y$ são os mesmos da regressão linear com multiplas variaveis matricial\n",
    "\n",
    "Na regressão logística a seguinte inequação deve ser seguida $0 \\leq h_\\theta(x) \\leq 1$.\n",
    "\n",
    "* Se $h(\\theta) \\geq 0.5 \\to \\hat y$ = 1\n",
    "\n",
    "* Se $h(\\theta) \\lt 0.5 \\to \\hat y$ = 0\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/323px-Logistic-curve.svg.png)\n",
    "\n",
    "\n",
    "Portanto sendo $g(z)=\\frac{1}{1+e^{-z}}$,$z=X\\cdot\\Theta$\n",
    "\n",
    "Então:\n",
    "\n",
    "* $h_\\theta(x)=g(z)$\n",
    "\n",
    "Contudo, o $J$ terá que mudar pois se não o algorítmo não irá convergir pelo fato do J não \n",
    "ser mais convexo.\n",
    "\n",
    "* $J(\\theta)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\text{custo}(h_\\theta(x^{(i)}),y^{(i)})$\n",
    "\n",
    "Onde $m$ é a quantidade de linhas.\n",
    "\n",
    "* $\\text{custo}(h_\\theta(x),y)=-y.\\log(h_\\theta(x))-(1-y).\\log(1-(h_\\theta(x)))$\n",
    "\n",
    "E o resto algorítmo continua o mesmo da Regressão Linear na sua forma matricial!\n",
    "\n",
    "A **Acurácia** é a média de todos os resultados corretos e mostra quanto o nosso modelo está acertando. A mesma pode ser descrita como:\n",
    "\n",
    " * $acc=\\frac{1}{m}\\sum\\limits_{i=0}^{num\\_it\\_train}\\text{corretos}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redes Neurais**\n",
    "\n",
    "*Variáveis em letra maíuscula representa um **vetor** e letra minúscula um **escalar**.*\n",
    "\n",
    "Para Redes Neurais, utilizaremos a seguinte equação\n",
    "\n",
    "$Z = W \\cdot X + b$\n",
    "\n",
    "* Os **pesos** eram representados por $\\Theta$, agora é representado por $W$.\n",
    "\n",
    "* $X$ são os dades de entrada da rede.\n",
    "\n",
    "* b(bias) é o viés da rede, ou seja, é como um intercepto adicionado em uma equação linear. É um parâmetro adicional na Rede Neural que é usado para ajustar a saída junto com a soma ponderada das entradas no neurônio. Além disso, o valor da polarização permite mudar a função de ativação para a direita ou esquerda.\n",
    "\n",
    "Partido da seguinte Rede Neural:\n",
    "\n",
    "![](imgs/graphviz.png)\n",
    "\n",
    "*Obs: A quantidade de camadas de uma Rede Neural é contabilizada a partir da quantidade de camadas desconsiderando a entrada, ou seja, temos duas camadas nesse exemplo, uma oculta e uma de saída.* \n",
    "\n",
    "$X1$ e $X2$ são as entradas da rede, onde estes, por sua vez, são vetores.\n",
    "\n",
    "Onde $n1, n2, n3$ e $nF$ representa o neurônio 1, neurônio 2, neurônio 3 e neurônio final(saída da rede), respectivamente.\n",
    "\n",
    "Tanto na Regressão Linear quanto na Logística, tinhamos que uma determinada entrada($X$) era representada com a dimensão $m x n$, onde $m$ era a quantidade de varíaveis desse vetor, ou seja, cada **linha** era um dado diferente.\n",
    "\n",
    "Porém, em Redes Neurais, temos que os mesmos dados são utilizados de forma **Transposta**.\n",
    "\n",
    "**A dimensão dos cada vetores de entrada é dada da seguinte maneira:** \n",
    "\n",
    "Para $W^{[1]}$, temos que a quantidade de linhas é a quantidade de neurônios na camada oculta e as colunas desse vetor será a quantidade de entradas na rede!\n",
    "\n",
    "* $W^{[1]} \\rightarrow$ qtd de neurônios na camada oculta x quantidade de variáveis de entrada.\n",
    "* $W^{[1]} \\rightarrow 3 x 2$.\n",
    "\n",
    "$\n",
    "W^{[1]}=\\begin{bmatrix}\n",
    "   W_1^{[1]} && W_1^{[2]} \\\\\n",
    "   W_2^{[1]} && W_2^{[2]} \\\\\n",
    "   W_3^{[1]} && W_3^{[2]} \\\\\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "Para $b^{[1]}$, temos que este sempre será um vetor coluna! Ou seja, $nx1$, onde $n$ é a quantidade de neurônios na camada oculta.\n",
    "\n",
    "$n= $*Quantidade de neurônios na camada oculta(linhas);*\n",
    "\n",
    "* $b^{[1]} \\rightarrow$ qtd de neurônios na camada oculta x 1.\n",
    "* $b^{[1]} \\rightarrow 3 x 1$.\n",
    "\n",
    "$b^{[1]}=\\begin{bmatrix}\n",
    "       b_1^{[1]} \\\\\n",
    "       b_2^{[1]} \\\\\n",
    "       b_3^{[1]} \\\\\n",
    "   \\end{bmatrix}$\n",
    "\n",
    "Por fim, temos que $X$ tem dimensão $kxm$, onde cada **linha** é uma entrada, ou seja, um conjunto de dados diferente e cada **coluna** é uma variável.\n",
    "\n",
    "$k= $*Quantidade de entradas(linhas);*\n",
    "\n",
    "$m = $*Tamanho da entrada(colunas);*\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "$X=\\begin{bmatrix}\n",
    "       X_1 \\\\\n",
    "       X_2 \\\\\n",
    "   \\end{bmatrix} \\rightarrow X=\\begin{bmatrix}\n",
    "                                   X_1^{[0]} && X_1^{[1]} && \\cdots && X_1^{[m]} \\\\\n",
    "                                   X_2^{[0]} && X_2^{[1]} && \\cdots && X_2^{[m]} \\\\\n",
    "                               \\end{bmatrix}$\n",
    "\n",
    "Partindo dessa mesma lógica, podemos perceber que, na segunda camada, é esperado que $W$ e $b$ tenham as seguintes dimensões:\n",
    "\n",
    "$W_{1xn}$ e $b_{1x1}$, ou seja, um **escalar**! Pois só temos **1** neurônio nessa camada!\n",
    "\n",
    "$\n",
    "W^{[2]}=\\begin{bmatrix}\n",
    "   W_1^{[1]} && W_1^{[2]} && W_1^{[3]}\\\\\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "### Propagation\n",
    "\n",
    "*Obs: Para este exemplo, a função de ativação que está sendo utilizada é a sigmóide e está sendo considerado que uma entrada possui 4 varíaveis.*\n",
    "\n",
    "A propagação é a fase na rede neural que é obtido a saída do modelo, ou seja, a saída do mesmo será $A^{[2]}$, nesse contexto, é mesma coisa que \n",
    "$g(Z) \\rightarrow \\hat{y} \\rightarrow H_{\\Theta}$, ou seja, é o que foi predito!\n",
    "\n",
    "$Z^{[1]}_{3x4} = W^{[1]}_{3x2} \\cdot X_{2x4} + B^{[1]}_{3x1}$\n",
    "\n",
    "$A^{[1]}_{3x4} = g^{[1]}(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]}_{1x4} = W^{[2]}_{1x3} \\cdot A^{[1]}_{3x4} + B^{[2]}_{1x1}$\n",
    "\n",
    "$A^{[2]}_{1x4} = g^{[2]}(Z^{[2]}_{1xm})$\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "A retropropagação é a fase onde atualiza o valor das variáveis da rede, no intuito de melhorar o resultado final. Após calcular o loss na fase de progação, é possível melhorar o resultado do modelo. Essa fase consiste na aplicação da regra da cadeia(chain rule).\n",
    "\n",
    "O *loss* é calculado para saber o quão bom está o modelo e este é importante para ser utilizado na fase de retropropagação(backpropagation).\n",
    "\n",
    "$L(A^{[2]}_{1xm}, Y_{1xm}) = -Y \\cdot log (A^{[2]}) - (1 - Y) \\cdot log (1 - A^{[2]})$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial A^{[2]}} = A^{[2]} - Y$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial A^{[2]}} \\cdot A^{[1]T}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b^{[2]}} = \\frac{\\partial L}{\\partial A^{[2]}} \\cdot 1$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial A^{[2]}} \\cdot \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b^{[1]}} = \\frac{\\partial L}{\\partial A^{[2]}} \\cdot \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} \\cdot \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\cdot \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}}$\n",
    "\n",
    "$\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} = W^{[2]}$\n",
    "\n",
    "$\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} = X$\n",
    "\n",
    "$\\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} = 1$\n",
    "\n",
    "$\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} = A^{[1]} \\cdot (1 - A^{[1]}) \\rightarrow g^{[1]'}(Z^{[1]})$\n",
    "\n",
    "### Algoritmo\n",
    "\n",
    "Em redes neurais, não podemos inicializar os pesos com $0$, assim como fazíamos na regressão linear e logística.\n",
    "\n",
    "Portanto, os mesmos devem ser inicializados de forma randômica, com os valores próximos a 0.\n",
    "\n",
    "O $b$ deve ser inicializado como um vetor de $0$ de dimensão $nx1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eixos NumPy\n",
    "\n",
    "* **Axis 0 - Linha**; \n",
    "* **Axis 1 - Coluna**;\n",
    "\n",
    "**Operações por coluna = Axis 0, porque ele faz linha a linha sobre uma mesma coluna**\n",
    "\n",
    "**Operações por linha = Axis 1, porque ele faz coluna a coluna sobre uma mesma linha**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, se eu quero saber qual é a média de um valores de uma mesma coluna, eu devo realizar:\n",
    "    \n",
    "    np.mean(array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/gj5ue.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo, temos de pegar a média e o desvio padrão dos valores de cada coluna, que seria referente a um dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Normalização vai deixar os dados numa mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a=\\begin{bmatrix}\n",
    "        1 & 2 \\\\\n",
    "        3 & 4 \\\\\n",
    "    \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = 0 **SOMA DAS COLUNAS, OPERACAO LINHA A LINHA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(axis=0) #(1+3)/2 = 2 ||| (2+4)/2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = 1 **SOMA DAS LINHAS, OPERACAO COLUNA A COLUNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(axis=1) #(1+2)/2 = 1.5 ||| (3+4)/2 = 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1+2+3+4)/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOTAÇÕES Variadas\n",
    "\n",
    "**ignorar esse tópico qqr coisa**\n",
    "\n",
    "verificar o balanceamento para o exemplo que fizemos em aula - (exemplo das notas)\n",
    "\n",
    "entradas positivas e negativas, ta balanceado no exemplo em aula(40,40), SE NAO TIVER BALANCEADO, existem métodos para balancear(ex: data aumentation - para imagens)\n",
    "\n",
    "Estudar: diferença da reg. linear vs reg. logistica\n",
    "\n",
    "cada matriz de cores é chamada de canal\n",
    "\n",
    "nossas imagens sao de 64x64, como essas sao colorias(r,g,b)\n",
    "\n",
    "teremos 3 canais diferentes, uma para o R, uma para o G e outra para o B.\n",
    "logo, 64 * 64 * 3 = 12288 - quantidade total de pixel(valores).\n",
    "\n",
    "4096 * 3\n",
    "\n",
    "**FAZER EM ESCALA RGB**\n",
    "\n",
    "$\\theta_0$ seria o cara que multiplica vezes 1, por isso que começamos no $\\theta_1$\n",
    "\n",
    "nesse exemplo teriamos 4096 thetas para o canal 1, 4096 thetas para o canal 2 e 4096 thetas para o canal 3,\n",
    "no caso $\\theta_1$ ate $\\theta_{4096}$, $\\theta_{4097}$ ate $\\theta_{8192}$, $\\theta_{4097}$ ate $\\theta_{8192}$, $\\theta_{8193}$ ate $\\theta_{12288}$\n",
    "\n",
    "**POR QUE CONVERTER PARA ESCALA DE CINZA?**\n",
    "\n",
    "fica menor o intervalo de valores (0-255), e em vez de trabalhar com 3 canais, iremos trabalhar com somente um\n",
    "\n",
    "converter para cinza é uma ideia para plotar num histograma:\n",
    "\n",
    "histograma tem o formato (254,quantidade de valores)\n",
    "\n",
    "f(x) no hist é a frequencia que a cor de 0-255 aparece nas imagens, o tamanho da barra é a quantidade de vezes que essa cor aparece na imagem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
